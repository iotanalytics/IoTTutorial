{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastICA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObR6SzrEMj+NO+CPE7eQ2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iotanalytics/IoTTutorial/blob/main/code/detection_and_segmentation/FastICA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtysvSw6hFC6"
      },
      "source": [
        "# Fast Algorithm for Independent Component Analysis (FastICA)\n",
        "\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html\n",
        "\n",
        "FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.\n",
        "\n",
        "ICA is a dimensionality reduction approach, and could find the most salient information of the dataset. This is an unsupervised problem, and could detect anomalies without labelling them before. This approach could be adapted to newly emerging patterns of fraud as quickly as possible.\n",
        "\n",
        "ref:\n",
        "https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/ch04.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebDRanIuQNmB"
      },
      "source": [
        "In the context of our smart grid signal dataset, the algorithms will have the largest reconstruction error on those time points that are hardest to model in other words, those that occur the least often and are the most anomalous. Since cyber attack is rare and presumably different than normal signals, the anomaly time points should exhibit the largest reconstruction error. So let’s define the anomaly score as the reconstruction error. The reconstruction error for each signal at some time point is the sum of the squared differences between the original feature matrix and the reconstructed matrix using the dimensionality reduction algorithm. We will scale the sum of the squared differences by the max-min range of the sum of the squared differences for the entire dataset, so that all the reconstruction errors are within a zero to one range.\n",
        "\n",
        "The signal of some time point that have the largest sum of squared differences will have an error close to one, while those that have the smallest sum of squared differences will have an error close to zero.\n",
        "\n",
        "This approach could be used for detecting anomalies in multivariate time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-kq7ytYSX96"
      },
      "source": [
        "# Load datasets\n",
        "current_path = os.getcwd()\n",
        "file = '\\\\datasets\\\\credit_card_data\\\\credit_card.csv'\n",
        "data = pd.read_csv(current_path + file)\n",
        "\n",
        "dataX = data.copy().drop(['Class'],axis=1)\n",
        "dataY = data['Class'].copy()\n",
        "\n",
        "featuresToScale = dataX.columns\n",
        "sX = pp.StandardScaler(copy=True)\n",
        "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(dataX, dataY, test_size=0.33, \\\n",
        "                    random_state=2018, stratify=dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTs_ohEXSYtD"
      },
      "source": [
        "#### Define Anomaly Score Function\n",
        "#### Next, we need to define a function that calculates how anomalous each transaction is. \n",
        "#### The more anomalous the transaction is, the more likely it is to be fraudulent, assuming\n",
        "#### that fraud is rare and looks somewhat different than the majority of transactions, which are normal.\n",
        "\n",
        "def anomalyScores(originalDF, reducedDF):\n",
        "    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n",
        "    loss = pd.Series(data=loss,index=originalDF.index)\n",
        "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "    return loss\n",
        "\n",
        "def plotResults(trueLabels, anomalyScores, returnPreds = False):\n",
        "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
        "    preds.columns = ['trueLabel', 'anomalyScore']\n",
        "    precision, recall, thresholds = \\\n",
        "        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n",
        "    average_precision = \\\n",
        "        average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n",
        "\n",
        "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
        "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "\n",
        "    plt.title('Precision-Recall curve: Average Precision = \\\n",
        "    {0:0.2f}'.format(average_precision))\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n",
        "                                     preds['anomalyScore'])\n",
        "    areaUnderROC = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
        "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic: \\\n",
        "    Area under the curve = {0:0.2f}'.format(areaUnderROC))\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    if returnPreds==True:\n",
        "        return preds\n",
        "\n",
        "def scatterPlot(xDF, yDF, algoName):\n",
        "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
        "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
        "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
        "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
        "               data=tempDF, fit_reg=False)\n",
        "    ax = plt.gca()\n",
        "    ax.set_title(\"Separation of Observations using \"+algoName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVAtpn0gdiTP"
      },
      "source": [
        "# Independent Component Analysis\n",
        "\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "n_components = 27\n",
        "algorithm = 'parallel'\n",
        "whiten = True\n",
        "max_iter = 200\n",
        "random_state = 2018\n",
        "\n",
        "fastICA = FastICA(n_components=n_components, \\\n",
        "    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \\\n",
        "    random_state=random_state)\n",
        "\n",
        "X_train_fastICA = fastICA.fit_transform(X_train)\n",
        "X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)\n",
        "\n",
        "X_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)\n",
        "X_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \\\n",
        "                                       index=X_train.index)\n",
        "\n",
        "scatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}